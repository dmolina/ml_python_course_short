{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f75884",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In this notebook, we are going to tackle unsupervised learning. In this\n",
    "learning, there is not a \"right\" category. Our target is to extract common\n",
    "patterns from the data, grouping them in function of their similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f39889",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Clustering of unlabeled data can be performed with the module sklearn.cluster.\n",
    "\n",
    "We have several clustering methods.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9142d",
   "metadata": {},
   "source": [
    "Clustering algorithm comes in two variants: \n",
    "\n",
    "- A class, that implements the fit method to learn the clusters on train data.\n",
    "- A function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the labels_ attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e675f5",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-Means is the simpler clustering algorithm, the idea is to group together\n",
    "elements considering its distance.\n",
    "\n",
    "<img\n",
    "src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_assumptions_001.png\" width=\"50%\">\n",
    "\n",
    "This algorithms requires to indicate the number of clusters.\n",
    "\n",
    "The good behavior depends on the distribution of the instances.\n",
    "\n",
    "**Important:** As always that the distance is used, the data must be previously\n",
    "normalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fe572",
   "metadata": {},
   "source": [
    "First, we are going to use a synthetic problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cb932",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df17968",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = make_blobs(n_samples=200,\n",
    "                         centers=k,\n",
    "                         cluster_std=2.75,\n",
    "                         random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a540d",
   "metadata": {},
   "source": [
    "We are going to visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592f2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x=features[:,0], y=features[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903d683",
   "metadata": {},
   "source": [
    "Now, we are going to use K-Means to classify them. This algorithm define several\n",
    "centroids, each for one cluster, assign the instances to the centroids nearest,\n",
    "and in an iterative way, the centroids are optimized. \n",
    "\n",
    "<img\n",
    "src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1280%2F1*rwYaxuY-jeiVXH0fyqC_oA.gif&f=1&nofb=1\" width=\"50%\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82869569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans( init=\"random\", # Init randomly\n",
    "    n_clusters=k, # N-Cluster\n",
    "    n_init=10,  # Scikit-Learn apply the algorithm several time to be more robust\n",
    "    max_iter=300, # Iterations\n",
    "    random_state=42 # Random state\n",
    ")\n",
    "# First normalize\n",
    "features_norm = StandardScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we apply the kmeans\n",
    "kmeans.fit(features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5647a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the clusters\n",
    "clusters = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.relplot(x=features_norm[:,0], y=features_norm[:,1], color='gray')\n",
    "\n",
    "for i in range(k):\n",
    "    sns.scatterplot(x=[clusters[i,0]],y=[clusters[i,1]], ax=plot.ax, marker='^', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317bfc8",
   "metadata": {},
   "source": [
    "Now we are going to assign each point with its cluster. They are assigned in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143097ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98758baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88270bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = np.array(features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ba63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotpoints(labels):\n",
    "    for i in range(k):\n",
    "        posi = (labels==i)\n",
    "        sns.scatterplot(x=features_np[posi,0],y=features_np[posi,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpoints(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3d903",
   "metadata": {},
   "source": [
    "## Measures\n",
    "\n",
    "In this case, there is not a right solution, so it has no sense to use a measure\n",
    "as accuracy or f1. In this case, we use another metrics like the **silhouette\n",
    "coefficient**. Sihoette is a measure of cluster cohesion and separation. \n",
    "\n",
    "It quantifies how well a data point fits into its assigned cluster based on two\n",
    "factors:\n",
    "\n",
    "- Inter-cluster distance: How close the data point is to other points in the cluster\n",
    "- Intra-cluster distance: How far away the data point is from points in other\n",
    "  clusters\n",
    "  \n",
    "The objective is to maintain the inter-cluster distance low, and a large\n",
    "intra-cluster distance.\n",
    "\n",
    "Silhouette coefficient values range between -1 and 1.  Larger numbers indicate\n",
    "that samples are closer to their clusters than they are to other clusters, so\n",
    "the goal is to minimize that measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(features_np, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecd060",
   "metadata": {},
   "source": [
    "Is it a good or a bad value? We are going to show how it changes when the k changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765f833",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(init=\"random\", n_clusters=k, n_init=10, max_iter=300, random_state=42)\n",
    "    kmeans.fit(features_np)\n",
    "    measures.append(silhouette_score(features_np, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    disp = sns.relplot(x=range(2, 11), y=measures,  kind='line')\n",
    "ax = disp.ax\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"Silhouette\")\n",
    "ax.set_title(\"Silhouette Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3bb9c",
   "metadata": {},
   "source": [
    "It can be seen that it reduces when k increases, but it is important to find a k\n",
    "value for which the reduction is important but at the same time k is lower. This\n",
    "is important because a low k allow us to interpret better the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae22e37f",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Another interesting clustering algorithm is DBSCAN. In many situations it can\n",
    "make a more natural clustering.\n",
    "\n",
    "<img\n",
    "src=\"https://files.realpython.com/media/crescent_comparison.7938c8cf29d1.png\"\n",
    "width=\"50%\"> \n",
    "\n",
    "From the scikit-learn User Guide: The DBSCAN algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, min_samples and eps, which define formally what we mean when we say dense. Higher min_samples or lower eps indicate higher density necessary to form a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2d279",
   "metadata": {},
   "source": [
    "To use it is very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a073b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.3) #  eps is the maximum distance between two samples for one to be considered as in the neighborhood of the other. \n",
    "\n",
    "dbscan.fit(features_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a6410",
   "metadata": {},
   "source": [
    "Now we are going to show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af592af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpoints(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412a290",
   "metadata": {},
   "source": [
    "In this case, it make only two clusters. We will see silhouette score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd0188",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(features_np, dbscan.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431f818",
   "metadata": {},
   "source": [
    "With only two clusters, it has improve the measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d36beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan2 = DBSCAN(eps=0.25, min_samples=3)\n",
    "dbscan2.fit(features_np)\n",
    "plotpoints(dbscan2.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1344f3d",
   "metadata": {},
   "source": [
    "There are two parameters to the algorithm, *min_samples* and *eps*, which define\n",
    "formally what we mean when we say dense. Higher *min_samples* or lower *eps*\n",
    "indicate higher density necessary to form a cluster. *min_samples* allow\n",
    "algorithm to be more robust against noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8e95f",
   "metadata": {},
   "source": [
    "**Task**: Change eps and min_sample and observe the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a33f97",
   "metadata": {},
   "source": [
    "## Another algorithm: MeanShift, OPTICS\n",
    "\n",
    "OPTICS is very similar to DBSCAN, but it presents a better behavior with large n_samples and large n_clusters. \n",
    "\n",
    "MeanShift clustering aims to discover blobs in a smooth density of samples. It\n",
    "is a centroid based algorithm, which works by updating candidates for centroids\n",
    "to be the mean of the points within a given region. These candidates are then\n",
    "filtered in a post-processing stage to eliminate near-duplicates to form the\n",
    "final set of centroids.\n",
    "\n",
    "**MeanShift** requires a parameter *bandwidth* but it can be estimated through\n",
    "function **estimate_bandwidth**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b97165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following bandwidth can be automatically estimated\n",
    "bandwidth = estimate_bandwidth(features_np, quantile=0.2, n_samples=500)\n",
    "\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(features_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208576a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpoints(ms.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203277a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(features_np, ms.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4faa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Task**: Change quantile to observe the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dba2af",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "In this type of clustering, instances closer are grouped in the same clusters,\n",
    "creating many small clusters. Then, they are hierarchical grouping between them\n",
    "in different levels.\n",
    "\n",
    "In particular, **AgglomerativeClustering** performs a hierarchical clustering\n",
    "using a bottom up approach: each observation starts in its own cluster, and\n",
    "clusters are successively merged together.\n",
    "\n",
    "The linkage criteria determines the metric used for the merge strategy:\n",
    "\n",
    "- **ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n",
    "\n",
    "- **complete** linkage minimizes the maximum distance between observations of pairs of clusters.\n",
    "\n",
    "- **average** linkage minimizes the average of the distances between all observations of pairs of clusters.\n",
    "\n",
    "- **Single** linkage minimizes the distance between the closest observations of pairs of clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98829a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(linkage='ward', n_clusters=3).fit(features_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81244548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotpoints(clustering.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(features_np, clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6b3d2",
   "metadata": {},
   "source": [
    "We are going to show the hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3e861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_threshold implies to calculate all distance\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0).fit(features_np)\n",
    "plt.figure(figsize=(100,100))\n",
    "plot_dendrogram(clustering, p=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
